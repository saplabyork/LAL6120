---
title: "Hillenbrand et al. (1995)"
output: 
  html_document:
    includes:
      in_header: "favicon.html"
    theme: paper
    toc: true
    toc_float: true
    collapsed: false
    number_sections: false
    toc_depth: 2
    code_folding: hide
---
<style type="text/css">
  body{
  font-size: 12pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev = "png",
                      dpi = 600,
                      echo = FALSE,
                      cache = FALSE)
options(repos = list(CRAN="http://cran.rstudio.com/"))
install.packages('tidyverse') 
library(tidyverse)
```

# Hillenbrand, Getty, Clark, and Wheeler (1995)

This paper (is now old) but was pretty influential at the time because it aimed to replicate a (much) earlier paper by [Peterson](images/gordon-peterson.jpeg) and Barney (1952) that exemplified one of the *problems of speech perception*, namely that phonological object (in this case a vowel) can have multiple acoustic realizations both within and between speakers (of varying age and gender). P&B recorded and analyzed canonical American English vowels produced in h-V-d contexts as produced by men, women, and children. The resulting image has been used in countless descriptions of the complexity of vowel spaces:

<p align="center">
  <img src="images/PetersonBarney52.png">
</p>

Hillbrand attempts to revisit P&B. Note that Hillenbrand is in central Michigan, which is quite different from the data P&B collected near the Bell Laboratories in New Jersey.

## P&B's measurements
- F1-F3 (Hz)
- F1-F3 (amplitude)
- F0

All measurements made from the "steady state" portion of the vowel. Why? $\rightarrow$ The middle section of a vowel is thought to be least affected by neighbouring segments (i.e., least coarticulation).

Let's do a quick exercise examining vowels and variability. Download the following three .wav files. Each has a list of hVd words in a carrier sentence. The vowels are: "hawed," "heed," "hood" (/u/ vowel), "had," "head," and "hud."

[hVd1](sounds/hvd1.wav)
[hvd2](sounds/hvd2.wav)
[hvd3](sounds/hvd3.wav)

Cut up these files and make vowel midpoint measurements (F1 and F2) for the target sounds. Then input your measurements into an Excel document and then save it as a .csv file to be read into R. 

<p align="center">
  <img src="images/hawed.png" width=50% height=50%>
</p>

Here are my measurements of F1, F2 at the vowel midpoints $\rightarrow$ [data](hvd_data.csv)

```{r echo=FALSE}
hvd <- read_csv('hvd_data.csv')
hvd
```

Now we can plot a rudimentary **scatterplot** of these data:

```{r echo=TRUE}
ggplot(hvd, aes(x=F2, y=F1, label = vowel)) + geom_text() + scale_x_reverse() + scale_y_reverse()
```

Can we look at the data in any other way? How about boxplots? A boxplot shows mean values of each group with some variance statistics (error bars, quartile ranges, etc.). It might be not very useful for the present data but let's try it nonetheless.

```{r echo=TRUE}
ggplot(hvd, aes(vowel, F2-F1)) + geom_boxplot()
```

# Assignment 1

You can download Assignment 1 [here](docs/LAL6120_Assignment1.pdf). You'll need some way to record your voice onto your computer so that you can analyze in Praat and then make some plots in R. 